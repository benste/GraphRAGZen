<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLM interaction &mdash; GraphRAGZen 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/css/sphinx_rtd_size.css?v=8f58cd29" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx_collapse.css?v=226d88b4" />

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../_static/jquery.js?v=5d32c60e"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../_static/documentation_options.js?v=01f34227"></script>
        <script src="../_static/doctools.js?v=9a2dae69"></script>
        <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            GraphRAGZen
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Content</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/index.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../motivation/index.html">Motivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../graphzen_ideology/index.html">GraphRAGZen ideology</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html">Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../todo/index.html">TODO</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prompts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prompts/default_prompts/index.html">Default Prompts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prompts/prompt_tuning/index.html">Prompt tuning Prompts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api/graphragzen.html">graphragzen</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">GraphRAGZen</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          























<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>LLM interaction</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/graphragzen_concepts/LLM_interaction.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="llm-interaction">
<span id="llm-interaction-label"></span><h1>LLM interaction<a class="headerlink" href="#llm-interaction" title="Link to this heading"></a></h1>
<p><strong>GraphRAGZen</strong> uses an LLM for the following:</p>
<ul class="simple">
<li><p>Creating custom prompts for extracting graphs. These prompts are specific to the domain of your documents .</p></li>
<li><p>Extracting a graph from your documents.</p></li>
<li><p>Summarizing a node or edge feature that has multiple descriptions of that feature. <span class="raw-html"><br /></span> (this happens when a node or edge is found multiple times in your documents. The features assigned to this entity are concatenated when creating the graph, where the entity exists only once.)</p></li>
<li><p>Creating descriptions of graph clusters.</p></li>
<li><p>Querying while adding context from the graph.</p></li>
</ul>
<p>Two methods are supported to interact with an LLM:</p>
<ol class="arabic simple">
<li><p>By loading a model locally in-memory.</p></li>
<li><p>With an LLM running on a server through an openAI API compatible endpoint.</p></li>
</ol>
<p>A server can be remote or deployed locally depending on your own preference.</p>
<p>Loading a model in-memory uses llama-cpp-python and is unlikely to use your GPU unless configured well. Thus, using locally in-memory is good for development and testing, but for production deployment it is recommended to communicate with an LLM that is properly set-up on a server.</p>
<p>These examples show how the different ways to initiate interaction with an LLM</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphragzen.llm</span> <span class="kn">import</span> <span class="n">OpenAICompatibleClient</span>
<span class="kn">from</span> <span class="nn">graphragzen.llm</span> <span class="kn">import</span> <span class="n">Phi35MiniGGUF</span>


<span class="c1"># Using OpenAI&#39;s API</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAICompatibleClient</span><span class="p">(</span>
    <span class="n">api_key_env_variable</span> <span class="o">=</span> <span class="s2">&quot;OPENAI_API_KEY&quot;</span>  <span class="c1"># the env variable, not the actual key!</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
    <span class="n">context_size</span> <span class="o">=</span> <span class="mi">32768</span><span class="p">,</span>
    <span class="c1"># Caches responses locally, saving time and OpenAI credits if the same request is made twice</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># Append the cache to a file on disk so it can be re-used between runs.</span>
    <span class="n">cache_persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># The file to store the persistent cache.</span>
    <span class="n">persistent_cache_file</span><span class="o">=</span><span class="s2">&quot;./OpenAI_persistent_cache.yaml&quot;</span>
<span class="p">)</span>

<span class="c1"># If you&#39;re running your own server running an LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">OpenAICompatibleClient</span><span class="p">(</span>
    <span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8081&quot;</span><span class="p">,</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;joeAI/phi3.5:latest&quot;</span>
    <span class="n">context_size</span> <span class="o">=</span> <span class="mi">32768</span><span class="p">,</span>
    <span class="c1"># Caches responses locally, saving time and server load if the same request is made twice</span>
    <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># Append the cache to a file on disk so it can be re-used between runs.</span>
    <span class="n">cache_persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="c1"># The file to store the persistent cache.</span>
    <span class="n">persistent_cache_file</span><span class="o">=</span><span class="s2">&quot;./phi35_mini_instruct_server_persistent_cache.yaml&quot;</span>
<span class="p">)</span>

<span class="c1"># Load model locally in-memory using llama-cpp-python</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">Phi35MiniGGUF</span><span class="p">(</span>
    <span class="n">model_storage_path</span><span class="o">=</span><span class="s2">&quot;path/to/Phi-3.5-mini-instruct-Q4_K_M.gguf&quot;</span><span class="p">,</span>
    <span class="n">tokenizer_URI</span><span class="o">=</span><span class="s2">&quot;microsoft/Phi-3.5-mini-instruct&quot;</span><span class="p">,</span>
    <span class="n">context_size</span><span class="o">=</span><span class="mi">32786</span><span class="p">,</span>
    <span class="n">persistent_cache_file</span><span class="o">=</span><span class="s2">&quot;./phi35_mini_instruct_persistent_cache.yaml&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<section id="which-model-to-use">
<h2>Which model to use<a class="headerlink" href="#which-model-to-use" title="Link to this heading"></a></h2>
<p>The latest OpenAI models give very good results, but might be costly; even with a moderate amount of documents the number of LLM calls to create a graph get large.</p>
<p>For choosing an open-source model, I found Phi 3.5 mini instruct to give good results in my tests. It’s a relatively small model so deployment should be easy and inference fast. When querying your knowledge graph you can switch to a larger, more capable model if desired.</p>
<p>Though Phi 3.5 mini instruct worked well in my tests, the domain of your documents might show different results. I would advice to extract entities from a small set of your documents, check if the extraction makes sense, and try a different model if it doens’t. Pay attention that not just quality nodes are extracted, but also a good amount of edges.</p>
<p><a class="reference external" href="https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/blob/main/Phi-3.5-mini-instruct-Q4_K_M.gguf">Phi 3.5 mini instruct Q4 K M</a></p>
<p><a class="reference external" href="https://huggingface.co/bartowski/gemma-2-2b-it-GGUF/blob/main/gemma-2-2b-it-Q4_K_M.gguf">Gemma 2 2B it Q4 M</a></p>
<p><a class="reference external" href="https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/blob/main/gemma-2-9b-it-IQ4_XS.gguf">Gemma 2 9B it Q4 XS</a></p>
</section>
<section id="async-llm-calls">
<h2>Async LLM calls<a class="headerlink" href="#async-llm-calls" title="Link to this heading"></a></h2>
<p>When interacting with an LLM on a server, aync LLM calls can significantly speed up the graph generation process.</p>
<p>Functions that make LLM calls have a <cite>async_llm_calls</cite> parameter that, when set to True, will call the LLM async.</p>
<p>The follow functions support this feature:</p>
<ul class="simple">
<li><p>extract_raw_entities (<a class="reference internal" href="../api/graphragzen.entity_extraction.extract_entities.extract_raw_entities.html#graphragzen.entity_extraction.extract_entities.extract_raw_entities" title="graphragzen.entity_extraction.extract_entities.extract_raw_entities"><code class="xref py py-func docutils literal notranslate"><span class="pre">graphragzen.entity_extraction.extract_entities.extract_raw_entities()</span></code></a>)</p></li>
<li><p>describe_clusters (<a class="reference internal" href="../api/graphragzen.clustering.describe.describe_clusters.html#graphragzen.clustering.describe.describe_clusters" title="graphragzen.clustering.describe.describe_clusters"><code class="xref py py-func docutils literal notranslate"><span class="pre">graphragzen.clustering.describe.describe_clusters()</span></code></a>)</p></li>
<li><p>generate_entity_relationship_examples (<a class="reference internal" href="../api/graphragzen.prompt_tuning.entities.generate_entity_relationship_examples.html#graphragzen.prompt_tuning.entities.generate_entity_relationship_examples" title="graphragzen.prompt_tuning.entities.generate_entity_relationship_examples"><code class="xref py py-func docutils literal notranslate"><span class="pre">graphragzen.prompt_tuning.entities.generate_entity_relationship_examples()</span></code></a>)</p></li>
</ul>
<p>Of the LLM classes, only the <cite>OpenAICompatibleClient</cite> has async implemented, since it’s the only class that interacts with an LLM on a server.</p>
<p>When calling it directly for text completion, i.e. llm(‘input text to complete’), the class checks if it is called in an async context and calls the server async or sync accordingly.</p>
<p>For chat functionality there is an async version, see <a class="reference internal" href="../api/graphragzen.llm.openAI_API_client.OpenAICompatibleClient.html#graphragzen.llm.openAI_API_client.OpenAICompatibleClient.a_run_chat" title="graphragzen.llm.openAI_API_client.OpenAICompatibleClient.a_run_chat"><code class="xref py py-func docutils literal notranslate"><span class="pre">graphragzen.llm.openAI_API_client.OpenAICompatibleClient.a_run_chat()</span></code></a></p>
</section>
<section id="implementing-your-own-local-llm-class">
<h2>Implementing your own local LLM class<a class="headerlink" href="#implementing-your-own-local-llm-class" title="Link to this heading"></a></h2>
<p>If the server does not have an OpenAI API compatible endpoint, or you want to load an LLM locally without using llama-cpp-python, you can implement a custom LLM class.</p>
<p><strong>GraphRAGZen</strong> expects certain methods when calling an LLM. The abstract base class <cite>LLM</cite> defines
the required methods using &#64;abstractmethod; you should inherit from this class when implementing your own LLM implementation.</p>
<p>See <a class="reference internal" href="../api/graphragzen.llm.base_llm.LLM.html#graphragzen.llm.base_llm.LLM" title="graphragzen.llm.base_llm.LLM"><code class="xref py py-class docutils literal notranslate"><span class="pre">graphragzen.llm.base_llm.LLM</span></code></a> (click source)</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Ben Steemers.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>